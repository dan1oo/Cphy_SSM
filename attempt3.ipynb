{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd3b906",
   "metadata": {},
   "source": [
    "## SSM implementation + testing\n",
    "\n",
    "TEST - lily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadb46c",
   "metadata": {},
   "source": [
    "#### general notes\n",
    "\n",
    "- time series model (note for matplotlib animate visualization)\n",
    "- \"model\" how a state space evolves over time\n",
    "- each discrete time step âˆ†t corresponds to 1 input data (-> step method)\n",
    "\n",
    "\n",
    "### prof suggestions\n",
    "\n",
    "- tweak the $A$ matrix (i.e. padding certian positions with 0 to see which structures are more important than others, or pad 0s at 1,1, or n,n positions of the matrix to see something about long-range dependency)\n",
    "- possibly include stuff on matrix conditioning? (perfectly-conditioned matrix = unitary)\n",
    "\n",
    "#### other notes\n",
    "\n",
    "- explore how linear dynamics/physical systems are reflected by the SSM (S4)\n",
    "- maybe tweaking $A$ will give us a general idea of how RNN is different from SSM\n",
    "- if time permits, implement RNN from scratch and explain how the hidden state is processed diffferently\n",
    "\n",
    "Daniel from gc:\n",
    "- ...it is similar in format from an RNN except the hidden matrix in RNN is non linear and only trained by gradient descent, whereas state space model like reflects linear dynamics / a more physical system and is more structured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb72bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c509fe",
   "metadata": {},
   "source": [
    "#### Model implementation\n",
    "\n",
    "things I want to cover:\n",
    "- classification\n",
    "- sklearn metrics\n",
    "- dimensionality reduction\n",
    "- cross-validation\n",
    "- hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb7347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cd753",
   "metadata": {},
   "source": [
    "#### Model visualization\n",
    "\n",
    "ideas to start off w/ (in-progress):\n",
    "\n",
    "- plot loss function over iterations\n",
    "- visualize parameter updates\n",
    "- matplotlib/seaborn\n",
    "    - attention heatmaps (extract attension weights from model + plot)\n",
    "- Model architecture/internal states\n",
    "    - Inspectus: used for ML visualization, explore model internals\n",
    "    - or just matplotlib (e.g. check activation patterns across model layers/components)\n",
    "- efficiency-focused visualizations\n",
    "    - LTSF-Linear: visualization of weights after training, get insight to predicted values\n",
    "    - OptimusPy: (check what this is)\n",
    "    - PPT (Token Pruning & Pooling Transformers): (check what this is)\n",
    "\n",
    "NOTE: We could do similar thing as Gu's Figure 2 (trained S4 model on LRA Path-X, where SSM convolution kernels are reshaped into a 128x128 image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b44bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
